{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparision of 4 different sentence segmentators for Russian language:\n",
    "* nltk.sent_tokenize without language argument (which uses \"english\" by default)\n",
    "* nltk.sent_tokenize with language='russian' (donwload russian.pkl from https://github.com/mhq/train_punkt )\n",
    "* a sentence segmentator from https://github.com/bureaucratic-labs/models (`pip install b-labs-models`)\n",
    "* our `ru_sent_tokenize`\n",
    "\n",
    "Following datasets are used to calculate metrics on:\n",
    "\n",
    "* OpenCorpora (downloaded http://opencorpora.org/files/export/annot/annot.opcorpora.xml.bz2 and extract)\n",
    "* SynTagRus (download *.conllu files from https://github.com/UniversalDependencies/UD_Russian-SynTagRus and put them to a folder)\n",
    "\n",
    "Modify `OPENCORP_FILE` and `SYNTAGRUS_DIR` constants in the next cell\n",
    "\n",
    "You will need about 16 GB RAM to run this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "from lxml import etree\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "\n",
    "import nltk\n",
    "from b_labs_models import SentenceSegmentator\n",
    "from ru_sent_tokenize import ru_sent_tokenize\n",
    "\n",
    "OPENCORPORA = 'opencorpora'\n",
    "SYNTAGRUS = 'syntagrus'\n",
    "\n",
    "OPENCORP_FILE = '/data/annot.opcorpora.xml'\n",
    "SYNTAGRUS_DIR = Path('/data/SynTagRus')\n",
    "RE_ENDS_WITH_PUNCT = re.compile(r\".*\\W$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 61889 sentences from syntagrus\n",
      "CPU times: user 4.52 s, sys: 20 ms, total: 4.54 s\n",
      "Wall time: 4.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "_monolit_syntagrus = []\n",
    "_compound_syntagrus = []\n",
    "\n",
    "CONLLU_TEXTS_RE = re.compile(r'# sent_id = (.*)_(\\d+)\\n# text = (.*)\\n', re.M)\n",
    "for fn in SYNTAGRUS_DIR.glob('*.conllu'):\n",
    "    with fn.open() as f:\n",
    "        txt = f.read()        \n",
    "\n",
    "    for g, data_iter in groupby((x.groups() for x in CONLLU_TEXTS_RE.finditer(txt)), key=itemgetter(0)):\n",
    "        data = sorted(data_iter, key=itemgetter(1))\n",
    "        for (_, _, s1) in data:\n",
    "            _monolit_syntagrus.append(s1)\n",
    "\n",
    "        for (_, _, s1), (_, _, s2) in zip(data[:-1], data[1:]):\n",
    "            if RE_ENDS_WITH_PUNCT.match(s1):\n",
    "                _compound_syntagrus.append((s1, s2))\n",
    "                \n",
    "print(f'Read {len(_monolit_syntagrus)} sentences from {SYNTAGRUS}')\n",
    "\n",
    "del txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 108959 sentences from opencorpora\n",
      "CPU times: user 8.65 s, sys: 1.91 s, total: 10.6 s\n",
      "Wall time: 10.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sents = list(etree.parse(OPENCORP_FILE).xpath('//source/text()'))\n",
    "\n",
    "_monolit_oc = []\n",
    "_compound_oc = []\n",
    "for s1, s2 in zip(sents[:-1], sents[1:]):\n",
    "    _monolit_oc.append(s1.strip())\n",
    "    if RE_ENDS_WITH_PUNCT.match(s1) and not s1.strip().endswith(':') and not s2.strip().startswith('—'):\n",
    "        _compound_oc.append([s1.strip(), s2.strip()])\n",
    "        \n",
    "print(f'Read {len(_monolit_oc)} sentences from {OPENCORPORA}')\n",
    "        \n",
    "del sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "_valid_dataset_names = {OPENCORPORA, SYNTAGRUS}\n",
    "def check_sent_tokenizer(tokenizer, dataset=OPENCORPORA):\n",
    "    assert dataset in _valid_dataset_names, \"dataset can be one of {}\".format(_valid_dataset_names)\n",
    "    \n",
    "    monoset = _monolit_oc if dataset == OPENCORPORA else _monolit_syntagrus\n",
    "    biset = _compound_oc if dataset == OPENCORPORA else _compound_syntagrus\n",
    "    correct_count_mono = 0\n",
    "    for m in monoset:\n",
    "        correct_count_mono += len(tokenizer(m)) == 1\n",
    "\n",
    "    correct_count_comp = 0\n",
    "    for s1, s2 in biset:\n",
    "        correct_count_comp += tokenizer(s1 + ' ' + s2) == [s1, s2]\n",
    "\n",
    "    return correct_count_mono / len(monoset), correct_count_comp / len(biset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.54 s, sys: 959 µs, total: 8.54 s\n",
      "Wall time: 8.54 s\n",
      "nltk.sent_tokenizer scores: 94.30%, 86.06%\n",
      "CPU times: user 5.09 s, sys: 271 µs, total: 5.09 s\n",
      "Wall time: 5.09 s\n",
      "nltk.sent_tokenizer scores: 98.15%, 94.95%\n"
     ]
    }
   ],
   "source": [
    "%time m, c = check_sent_tokenizer(nltk.sent_tokenize, OPENCORPORA)\n",
    "print(f'nltk.sent_tokenizer scores: {m*100:.2f}%, {c*100:.2f}%')\n",
    "print()\n",
    "%time m, c = check_sent_tokenizer(nltk.sent_tokenize, SYNTAGRUS)\n",
    "print(f'nltk.sent_tokenizer scores: {m*100:.2f}%, {c*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.51 s, sys: 16.4 ms, total: 8.53 s\n",
      "Wall time: 8.53 s\n",
      "nltk.sent_tokenizer scores: 95.53%, 88.37%\n",
      "CPU times: user 5.08 s, sys: 31 µs, total: 5.08 s\n",
      "Wall time: 5.08 s\n",
      "nltk.sent_tokenizer scores: 98.44%, 95.45%\n"
     ]
    }
   ],
   "source": [
    "%time m, c = check_sent_tokenizer(lambda s: nltk.sent_tokenize(s, 'russian'), OPENCORPORA)\n",
    "# donwload from https://github.com/mhq/train_punkt russian.pkl\n",
    "print(f'nltk.sent_tokenizer scores: {m*100:.2f}%, {c*100:.2f}%')\n",
    "print()\n",
    "%time m, c = check_sent_tokenizer(lambda s: nltk.sent_tokenize(s, 'russian'), SYNTAGRUS)\n",
    "# donwload from https://github.com/mhq/train_punkt russian.pkl\n",
    "print(f'nltk.sent_tokenizer scores: {m*100:.2f}%, {c*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 34s, sys: 1.01 s, total: 5min 35s\n",
      "Wall time: 5min 35s\n",
      "loose_sent_tokenizer scores: 97.16%, 88.62%\n",
      "\n",
      "CPU times: user 3min 18s, sys: 124 ms, total: 3min 18s\n",
      "Wall time: 3min 18s\n",
      "loose_sent_tokenizer scores: 96.79%, 92.55%\n"
     ]
    }
   ],
   "source": [
    "segmentator = SentenceSegmentator()\n",
    "%time m, c = check_sent_tokenizer(lambda x: list(segmentator.split(x)), OPENCORPORA)\n",
    "print(f'loose_sent_tokenizer scores: {m*100:.2f}%, {c*100:.2f}%')\n",
    "print()\n",
    "%time m, c = check_sent_tokenizer(lambda x: list(segmentator.split(x)), SYNTAGRUS)\n",
    "print(f'loose_sent_tokenizer scores: {m*100:.2f}%, {c*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.79 s, sys: 0 ns, total: 4.79 s\n",
      "Wall time: 4.79 s\n",
      "loose_sent_tokenizer scores: 98.83%, 93.19%\n",
      "\n",
      "CPU times: user 2.81 s, sys: 0 ns, total: 2.81 s\n",
      "Wall time: 2.81 s\n",
      "loose_sent_tokenizer scores: 99.82%, 96.56%\n"
     ]
    }
   ],
   "source": [
    "%time m, c = check_sent_tokenizer(ru_sent_tokenize, OPENCORPORA)\n",
    "print(f'loose_sent_tokenizer scores: {m*100:.2f}%, {c*100:.2f}%')\n",
    "print()\n",
    "%time m, c = check_sent_tokenizer(ru_sent_tokenize, SYNTAGRUS)\n",
    "print(f'loose_sent_tokenizer scores: {m*100:.2f}%, {c*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_tf18",
   "language": "python",
   "name": ".venv_tf18"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
